#!/usr/bin/env python3
"""
ä»PDFä¸­æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å¹´ä»½ï¼‰å¹¶ç”Ÿæˆåˆç†çš„æ–‡ä»¶å
"""
import re
import json
import os
from pathlib import Path
from datetime import datetime


def extract_title_from_pdf(pdf_path):
    """
    ä»PDFæå–æ ‡é¢˜ï¼ˆä¼˜å…ˆä»ç¬¬ä¸€é¡µæ–‡æœ¬ï¼Œå¤‡é€‰ä»metadataï¼‰

    è¿”å›: å®Œæ•´æ ‡é¢˜å­—ç¬¦ä¸²
    """
    try:
        import fitz  # PyMuPDF
    except ImportError:
        print("éœ€è¦å®‰è£…PyMuPDF: pip install pymupdf")
        return None

    doc = fitz.open(pdf_path)

    # æ–¹æ³•1ï¼šå°è¯•ä»PDF metadataè¯»å–
    metadata = doc.metadata
    if metadata and metadata.get('title') and len(metadata.get('title', '')) > 5:
        title = metadata['title']
        doc.close()
        return title.strip()

    # æ–¹æ³•2ï¼šä»ç¬¬ä¸€é¡µæå–ï¼ˆé€šå¸¸æ ‡é¢˜æ˜¯ç¬¬ä¸€é¡µæœ€å¤§å­—å·çš„æ–‡æœ¬ï¼‰
    if len(doc) > 0:
        first_page = doc[0]
        blocks = first_page.get_text("dict")["blocks"]

        # æ‰¾åˆ°å­—å·æœ€å¤§çš„æ–‡æœ¬å—ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜ï¼‰
        max_size = 0
        title_text = ""

        for block in blocks:
            if block.get("type") == 0:  # æ–‡æœ¬å—
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        size = span.get("size", 0)
                        text = span.get("text", "").strip()

                        # æ ‡é¢˜ç‰¹å¾ï¼šå­—å·å¤§ã€ä½äºé¡µé¢ä¸ŠåŠéƒ¨åˆ†ã€ä¸æ˜¯å•ä¸ªè¯
                        if size > max_size and len(text) > 10 and line.get("bbox", [0,0,0,0])[1] < 200:
                            max_size = size
                            title_text = text

        if title_text:
            doc.close()
            return title_text

    doc.close()
    return None


def extract_year_from_pdf(pdf_path, url=None):
    """
    æå–è®ºæ–‡å¹´ä»½

    ä¼˜å…ˆçº§ï¼š
    1. arxiv URL (å¦‚ https://arxiv.org/pdf/1910.10683 â†’ 2019)
    2. PDF metadata
    3. æ–‡ä»¶åˆ›å»ºæ—¶é—´

    è¿”å›: å¹´ä»½å­—ç¬¦ä¸² (å¦‚ "2019")
    """
    # æ–¹æ³•1ï¼šä»arxiv URLæå–
    if url and 'arxiv.org' in url:
        # arxivç¼–å·æ ¼å¼ï¼šYYMM.NNNNN æˆ– arch-ive/YYMMNNN
        match = re.search(r'/(\d{4})\.', url)
        if match:
            yymm = match.group(1)
            year = int(yymm[:2])
            # 2007å¹´åarxivç”¨4ä½å¹´ä»½ï¼Œä¹‹å‰ç”¨2ä½
            if year > 90:
                return f"19{year}"
            else:
                return f"20{year}"

    # æ–¹æ³•2ï¼šä»PDF metadata
    try:
        import fitz
        doc = fitz.open(pdf_path)
        metadata = doc.metadata
        doc.close()

        if metadata and metadata.get('creationDate'):
            # æ ¼å¼é€šå¸¸æ˜¯ "D:20191023..." æˆ– "D:2019..."
            date_str = metadata['creationDate']
            match = re.search(r'(\d{4})', date_str)
            if match:
                return match.group(1)
    except:
        pass

    # æ–¹æ³•3ï¼šæ–‡ä»¶åˆ›å»ºæ—¶é—´
    stat = os.stat(pdf_path)
    year = datetime.fromtimestamp(stat.st_ctime).year
    return str(year)


def simplify_title(title, max_length=30):
    """
    ç®€åŒ–æ ‡é¢˜ä¸ºçŸ­æ ‡è¯†

    ç­–ç•¥ï¼š
    1. æå–é¦–å­—æ¯ç¼©å†™ï¼ˆå¦‚ BERT, GPT, T5ï¼‰
    2. æå–å…³é”®è¯ï¼ˆTransfer Learning, Attentionï¼‰
    3. ç¿»è¯‘ä¸ºä¸­æ–‡å…³é”®è¯

    å‚æ•°:
        title: å®Œæ•´æ ‡é¢˜
        max_length: æœ€å¤§é•¿åº¦

    è¿”å›: ç®€åŒ–åçš„æ ‡è¯†ï¼ˆè‹±æ–‡ï¼Œé€‚åˆåšæ–‡ä»¶åï¼‰
    """
    # å¸¸è§ç¼©å†™æ¨¡å¼
    acronyms = re.findall(r'\b[A-Z][A-Z0-9]+\b', title)
    if acronyms:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªç¼©å†™
        return acronyms[0]

    # æå–å…³é”®è¯ï¼ˆå¤§å†™å¼€å¤´çš„è¯ï¼‰
    keywords = re.findall(r'\b[A-Z][a-z]+\b', title)
    if len(keywords) >= 2:
        # ç»„åˆå‰2-3ä¸ªå…³é”®è¯
        combined = '_'.join(keywords[:3])
        if len(combined) <= max_length:
            return combined

    # å¦‚æœæ²¡æœ‰åˆé€‚çš„å…³é”®è¯ï¼Œä½¿ç”¨å‰Nä¸ªå•è¯
    words = title.split()[:3]
    simplified = '_'.join(w for w in words if len(w) > 3)

    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
    simplified = re.sub(r'[^\w\-]', '_', simplified)
    simplified = re.sub(r'_+', '_', simplified)

    return simplified[:max_length]


def generate_paper_id(title, year, user_hint=None):
    """
    ç”Ÿæˆè®ºæ–‡æ ‡è¯†

    ä¼˜å…ˆçº§ï¼š
    1. ç”¨æˆ·æç¤ºï¼ˆå¦‚æœæä¾›ï¼‰
    2. æ ‡é¢˜ç®€åŒ– + å¹´ä»½

    å‚æ•°:
        title: å®Œæ•´æ ‡é¢˜
        year: å¹´ä»½
        user_hint: ç”¨æˆ·æä¾›çš„ç®€çŸ­æ ‡è¯†ï¼ˆå¯é€‰ï¼‰

    è¿”å›: paper_id (å¦‚ "T5_2019", "BERT_2018")
    """
    if user_hint:
        # æ¸…ç†ç”¨æˆ·è¾“å…¥
        clean_hint = re.sub(r'[^\w\-]', '_', user_hint)
        return f"{clean_hint}_{year}"

    # è‡ªåŠ¨ç”Ÿæˆ
    simplified = simplify_title(title)
    return f"{simplified}_{year}"


def extract_authors_from_pdf(pdf_path):
    """
    æå–ä½œè€…åˆ—è¡¨

    è¿”å›: ä½œè€…åˆ—è¡¨
    """
    try:
        import fitz
        doc = fitz.open(pdf_path)

        # ä»metadataæå–
        metadata = doc.metadata
        if metadata and metadata.get('author'):
            authors = metadata['author']
            doc.close()
            return [a.strip() for a in authors.split(',')]

        # ä»ç¬¬ä¸€é¡µæ–‡æœ¬æå–ï¼ˆæ ‡é¢˜ä¸‹æ–¹é€šå¸¸æ˜¯ä½œè€…ï¼‰
        # è¿™ä¸ªè¾ƒå¤æ‚ï¼Œæš‚æ—¶è·³è¿‡
        doc.close()
    except:
        pass

    return []


def create_metadata_json(pdf_path, url=None, user_hint=None):
    """
    åˆ›å»ºå®Œæ•´çš„å…ƒæ•°æ®JSON

    å‚æ•°:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        url: åŸå§‹URLï¼ˆå¯é€‰ï¼‰
        user_hint: ç”¨æˆ·æä¾›çš„ç®€çŸ­æ ‡è¯†ï¼ˆå¯é€‰ï¼‰

    è¿”å›: (paper_id, metadata_dict)
    """
    # æå–ä¿¡æ¯
    title = extract_title_from_pdf(pdf_path)
    year = extract_year_from_pdf(pdf_path, url)
    authors = extract_authors_from_pdf(pdf_path)
    paper_id = generate_paper_id(title or "Unknown", year, user_hint)

    # æ„å»ºå…ƒæ•°æ®
    metadata = {
        "paper_id": paper_id,
        "title": title or "Unknown",
        "year": year,
        "authors": authors,
        "source_url": url or "",
        "extracted_at": datetime.now().isoformat(),
        "original_filename": os.path.basename(pdf_path)
    }

    return paper_id, metadata


def organize_paper_directory(pdf_path, output_base="papers", url=None, user_hint=None):
    """
    ç»„ç»‡è®ºæ–‡ç›®å½•ç»“æ„

    å‚æ•°:
        pdf_path: åŸå§‹PDFè·¯å¾„
        output_base: è¾“å‡ºåŸºç¡€ç›®å½•
        url: åŸå§‹URL
        user_hint: ç”¨æˆ·æ ‡è¯†æç¤º

    è¿”å›: (paper_dir, paper_id, metadata)
    """
    # åˆ›å»ºå…ƒæ•°æ®
    paper_id, metadata = create_metadata_json(pdf_path, url, user_hint)

    # åˆ›å»ºç›®å½•
    paper_dir = os.path.join(output_base, paper_id)
    os.makedirs(paper_dir, exist_ok=True)
    os.makedirs(os.path.join(paper_dir, "images"), exist_ok=True)

    # å¤åˆ¶å¹¶é‡å‘½åPDF
    new_pdf_path = os.path.join(paper_dir, f"{paper_id}.pdf")
    if pdf_path != new_pdf_path:
        import shutil
        shutil.copy2(pdf_path, new_pdf_path)

    # ä¿å­˜å…ƒæ•°æ®
    metadata_path = os.path.join(paper_dir, "metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"âœ… è®ºæ–‡ç›®å½•å·²åˆ›å»ºï¼š{paper_dir}")
    print(f"   æ ‡é¢˜ï¼š{metadata['title'][:60]}...")
    print(f"   å¹´ä»½ï¼š{metadata['year']}")
    print(f"   PDFï¼š{paper_id}.pdf")
    print(f"   å…ƒæ•°æ®ï¼šmetadata.json")

    return paper_dir, paper_id, metadata


def main():
    """å‘½ä»¤è¡Œå·¥å…·"""
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python extract_pdf_metadata.py <PDFæ–‡ä»¶> [URL] [ç”¨æˆ·æ ‡è¯†]")
        print("ç¤ºä¾‹: python extract_pdf_metadata.py paper.pdf https://arxiv.org/pdf/1910.10683 T5")
        print("      python extract_pdf_metadata.py bert.pdf \"\" BERT")
        sys.exit(1)

    pdf_path = sys.argv[1]
    url = sys.argv[2] if len(sys.argv) > 2 and sys.argv[2] else None
    user_hint = sys.argv[3] if len(sys.argv) > 3 else None

    if not os.path.exists(pdf_path):
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        sys.exit(1)

    # ç»„ç»‡ç›®å½•
    paper_dir, paper_id, metadata = organize_paper_directory(
        pdf_path,
        output_base="papers",
        url=url,
        user_hint=user_hint
    )

    print(f"\nğŸ“ ç›®å½•ç»“æ„ï¼š")
    print(f"papers/")
    print(f"â””â”€â”€ {paper_id}/")
    print(f"    â”œâ”€â”€ {paper_id}.pdf")
    print(f"    â”œâ”€â”€ metadata.json")
    print(f"    â””â”€â”€ images/")


if __name__ == "__main__":
    main()
